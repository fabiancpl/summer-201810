{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( './data/stopwords.txt', 'r' ) as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [ x.strip() for x in stopwords ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_path = './data/abstracts/'\n",
    "abstracts_filenames = [ f for f in listdir( abstracts_path ) if isfile( join( abstracts_path, f ) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( abstracts_filenames )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_path = './data/gold/'\n",
    "gold_filenames = [ f for f in listdir( gold_path ) if isfile( join( gold_path, f ) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( gold_filenames )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames =  list( set( abstracts_filenames ) & set( gold_filenames ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1330"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( filenames )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for filename in filenames:\n",
    "    file = dict()\n",
    "    file[ 'filename' ] = filename\n",
    "    \n",
    "    with open( abstracts_path + filename, 'r' ) as tmp_file:\n",
    "        abstract = tmp_file.read().replace( '\\n', ' ' )\n",
    "    file[ 'abstract' ] = abstract\n",
    "    \n",
    "    with open( gold_path + filename, 'r' ) as tmp_file:\n",
    "        gold = tmp_file.readlines()\n",
    "    gold = [ w.replace('\\n', '') for w in gold ]\n",
    "    file[ 'gold' ] = gold\n",
    "    \n",
    "    files.append( file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1330"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( files )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&()*+,./:;<=>?@[\\\\]^`{|}~'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctation2 = punctuation.replace( '\\'', '' ).replace( '-', '' ).replace( '_', '' )\n",
    "punctation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = ( '_nn', '_nns', '_nnp', '_nnps', '_jj' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    words = ''.join( c.lower() for c in file[ 'abstract' ] if ( c not in punctation2 ) ).split()\n",
    "    words = [ w.split( '_' )[ 0 ] for w in words if w.endswith( suffixes ) ]\n",
    "    words = [ w for w in words if w not in stopwords ]\n",
    "    try:\n",
    "        words.remove( '' )\n",
    "    except ValueError:\n",
    "        pass\n",
    "    words = [ ps.stem( w ) for w in words ]\n",
    "    file[ 'tokenized_abstracts' ] = words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    graph = dict()\n",
    "    for i in range( len( file[ 'tokenized_abstracts' ] ) ):\n",
    "        token = file[ 'tokenized_abstracts' ][ i ]\n",
    "        if token not in graph:\n",
    "            graph[ token ] = dict()\n",
    "        try:\n",
    "            next_word = file[ 'tokenized_abstracts' ][ i + 1 ]\n",
    "            if next_word not in graph[ token ]:\n",
    "                graph[ token ][ next_word ] = 1\n",
    "            else:\n",
    "                graph[ token ][ next_word ] = graph[ token ][ next_word ] + 1 \n",
    "        except IndexError:\n",
    "            pass\n",
    "    file[ 'graph' ] = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 'Online_JJ spelling_NN correction_NN for_IN query_NN completion_NN In_IN this_DT paper_NN ,_, we_PRP study_VBD the_DT problem_NN of_IN online_JJ spelling_NN correction_NN for_IN query_NN completions_NNS ._. Misspelling_NNP is_VBZ a_DT common_JJ phenomenon_NN among_IN search_NN engines_NNS queries_NNS ._. In_IN order_NN to_TO help_VB users_NNS effectively_RB express_VBP their_PRP$ information_NN needs_NNS ,_, mechanisms_NNS for_IN automatically_RB correcting_VBG misspelled_VBN queries_NNS are_VBP required_VBN ._. Online_JJ spelling_NN correction_NN aims_VBZ to_TO provide_VB spell_NN corrected_JJ completion_NN suggestions_NNS as_IN a_DT query_NN is_VBZ incrementally_RB entered_VBN ._. As_IN latency_NN is_VBZ crucial_JJ to_TO the_DT utility_NN of_IN the_DT suggestions_NNS ,_, such_PDT an_DT algorithm_NN needs_VBZ to_TO be_VB not_RB only_RB accurate_JJ ,_, but_CC also_RB efficient_JJ ._. To_TO tackle_VB this_DT problem_NN ,_, we_PRP propose_VBP and_CC study_VBP a_DT generative_JJ model_NN for_IN input_NN queries_NNS ,_, based_VBN on_IN a_DT noisy_JJ channel_NN transformation_NN of_IN the_DT intended_JJ queries_NNS ._. Utilizing_VBG spelling_NN correction_NN pairs_NNS ,_, we_PRP train_VBP a_DT Markov_NNP n-gram_NN transformation_NN model_NN that_WDT captures_VBZ user_NN spelling_NN behavior_NN in_IN an_DT unsupervised_JJ fashion_NN ._. To_TO find_VB the_DT top_JJ spell-corrected_JJ completion_NN suggestions_NNS in_IN real-time_JJ ,_, we_PRP adapt_VBP the_DT A_NN \\\\*_NN search_NN algorithm_NN with_IN various_JJ pruning_NN heuristics_NNS to_TO dynamically_RB expand_VB the_DT search_NN space_NN efficiently_RB ._. Evaluation_NN of_IN the_DT proposed_VBN methods_NNS demonstrates_VBZ a_DT substantial_JJ increase_NN in_IN the_DT effectiveness_NN of_IN online_JJ spelling_NN correction_NN over_IN existing_VBG techniques_NNS ._. ',\n",
       " 'filename': '13061804',\n",
       " 'gold': ['a* search',\n",
       "  'query completion',\n",
       "  'spelling correction',\n",
       "  'transformation model'],\n",
       " 'graph': {'accur': {'effici': 1},\n",
       "  'algorithm': {'accur': 1, 'prune': 1},\n",
       "  'behavior': {'unsupervis': 1},\n",
       "  'channel': {'transform': 1},\n",
       "  'common': {'phenomenon': 1},\n",
       "  'complet': {'misspel': 1, 'paper': 1, 'suggest': 2},\n",
       "  'correct': {'complet': 1, 'pair': 1, 'queri': 2, 'spell': 1, 'techniqu': 1},\n",
       "  'crucial': {'util': 1},\n",
       "  'effect': {'onlin': 1},\n",
       "  'effici': {'problem': 1},\n",
       "  'engin': {'queri': 1},\n",
       "  'evalu': {'method': 1},\n",
       "  'fashion': {'top': 1},\n",
       "  'gener': {'model': 1},\n",
       "  'heurist': {'search': 1},\n",
       "  'increas': {'effect': 1},\n",
       "  'inform': {'mechan': 1},\n",
       "  'input': {'queri': 1},\n",
       "  'intend': {'queri': 1},\n",
       "  'latenc': {'crucial': 1},\n",
       "  'markov': {'n-gram': 1},\n",
       "  'mechan': {'queri': 1},\n",
       "  'method': {'substanti': 1},\n",
       "  'misspel': {'common': 1},\n",
       "  'model': {'input': 1, 'user': 1},\n",
       "  'n-gram': {'transform': 1},\n",
       "  'noisi': {'channel': 1},\n",
       "  'onlin': {'spell': 4},\n",
       "  'order': {'user': 1},\n",
       "  'pair': {'markov': 1},\n",
       "  'paper': {'problem': 1},\n",
       "  'phenomenon': {'search': 1},\n",
       "  'problem': {'gener': 1, 'onlin': 1},\n",
       "  'prune': {'heurist': 1},\n",
       "  'queri': {'complet': 2,\n",
       "   'latenc': 1,\n",
       "   'noisi': 1,\n",
       "   'onlin': 1,\n",
       "   'order': 1,\n",
       "   'spell': 1},\n",
       "  'real-tim': {'search': 1},\n",
       "  'search': {'algorithm': 1, 'engin': 1, 'space': 1},\n",
       "  'space': {'evalu': 1},\n",
       "  'spell': {'behavior': 1, 'correct': 6},\n",
       "  'spell-correct': {'complet': 1},\n",
       "  'substanti': {'increas': 1},\n",
       "  'suggest': {'algorithm': 1, 'queri': 1, 'real-tim': 1},\n",
       "  'techniqu': {},\n",
       "  'top': {'spell-correct': 1},\n",
       "  'transform': {'intend': 1, 'model': 1},\n",
       "  'unsupervis': {'fashion': 1},\n",
       "  'user': {'inform': 1, 'spell': 1},\n",
       "  'util': {'suggest': 1}},\n",
       " 'tokenized_abstracts': ['onlin',\n",
       "  'spell',\n",
       "  'correct',\n",
       "  'queri',\n",
       "  'complet',\n",
       "  'paper',\n",
       "  'problem',\n",
       "  'onlin',\n",
       "  'spell',\n",
       "  'correct',\n",
       "  'queri',\n",
       "  'complet',\n",
       "  'misspel',\n",
       "  'common',\n",
       "  'phenomenon',\n",
       "  'search',\n",
       "  'engin',\n",
       "  'queri',\n",
       "  'order',\n",
       "  'user',\n",
       "  'inform',\n",
       "  'mechan',\n",
       "  'queri',\n",
       "  'onlin',\n",
       "  'spell',\n",
       "  'correct',\n",
       "  'spell',\n",
       "  'correct',\n",
       "  'complet',\n",
       "  'suggest',\n",
       "  'queri',\n",
       "  'latenc',\n",
       "  'crucial',\n",
       "  'util',\n",
       "  'suggest',\n",
       "  'algorithm',\n",
       "  'accur',\n",
       "  'effici',\n",
       "  'problem',\n",
       "  'gener',\n",
       "  'model',\n",
       "  'input',\n",
       "  'queri',\n",
       "  'noisi',\n",
       "  'channel',\n",
       "  'transform',\n",
       "  'intend',\n",
       "  'queri',\n",
       "  'spell',\n",
       "  'correct',\n",
       "  'pair',\n",
       "  'markov',\n",
       "  'n-gram',\n",
       "  'transform',\n",
       "  'model',\n",
       "  'user',\n",
       "  'spell',\n",
       "  'behavior',\n",
       "  'unsupervis',\n",
       "  'fashion',\n",
       "  'top',\n",
       "  'spell-correct',\n",
       "  'complet',\n",
       "  'suggest',\n",
       "  'real-tim',\n",
       "  'search',\n",
       "  'algorithm',\n",
       "  'prune',\n",
       "  'heurist',\n",
       "  'search',\n",
       "  'space',\n",
       "  'evalu',\n",
       "  'method',\n",
       "  'substanti',\n",
       "  'increas',\n",
       "  'effect',\n",
       "  'onlin',\n",
       "  'spell',\n",
       "  'correct',\n",
       "  'techniqu']}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[ 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
